<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>readme</title>
    <style type="text/css">
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      span.underline {
        text-decoration: underline;
      }
      div.column {
        display: inline-block;
        vertical-align: top;
        width: 50%;
      }
    </style>
  </head>
  <body>
    <!--lint ignore double-link-->
    <h1 id="awesome-jax-awesome">
      Awesome JAX
      <a href="https://awesome.re"
        ><img src="https://awesome.re/badge.svg" alt="Awesome" /></a
      ><a href="https://github.com/google/jax"
        ><img
          src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png"
          alt="JAX Logo"
          align="right"
          height="100"
      /></a>
    </h1>
    <!--lint ignore double-link-->
    <p>
      <a href="https://github.com/google/jax">JAX</a> brings automatic
      differentiation and the
      <a href="https://www.tensorflow.org/xla">XLA compiler</a> together through
      a <a href="https://numpy.org/">NumPy</a>-like API for high performance
      machine learning research on accelerators like GPUs and TPUs.
      <!--lint enable double-link-->
    </p>
    <p>
      This is a curated list of awesome JAX libraries, projects, and other
      resources. Contributions are welcome!
    </p>
    <h2 id="contents">Contents</h2>
    <ul>
      <li><a href="#libraries">Libraries</a></li>
      <li><a href="#models-and-projects">Models and Projects</a></li>
      <li><a href="#videos">Videos</a></li>
      <li><a href="#papers">Papers</a></li>
      <li><a href="#tutorials-and-blog-posts">Tutorials and Blog Posts</a></li>
      <li><a href="#community">Community</a></li>
    </ul>
    <p><a name="libraries" /></p>
    <h2 id="libraries">Libraries</h2>
    <ul>
      <li>
        Neural Network Libraries
        <ul>
          <li>
            <a href="https://github.com/google/flax">Flax</a> - Centered on
            flexibility and clarity.
            <img
              src="https://img.shields.io/github/stars/google/flax?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/deepmind/dm-haiku">Haiku</a> - Focused
            on simplicity, created by the authors of Sonnet at DeepMind.
            <img
              src="https://img.shields.io/github/stars/deepmind/dm-haiku?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/google/objax">Objax</a> - Has an object
            oriented design similar to PyTorch.
            <img
              src="https://img.shields.io/github/stars/google/objax?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://poets-ai.github.io/elegy/">Elegy</a> - A
            framework-agnostic Trainer interface for the Jax ecosystem. Supports
            Flax, Haiku, and Optax.
            <img
              src="https://img.shields.io/github/stars/poets-ai/elegy?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/google/trax">Trax</a> - “Batteries
            included” deep learning library focused on providing solutions for
            common workloads.
            <img
              src="https://img.shields.io/github/stars/google/trax?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/deepmind/jraph">Jraph</a> - Lightweight
            graph neural network library.
            <img
              src="https://img.shields.io/github/stars/deepmind/jraph?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/google/neural-tangents"
              >Neural Tangents</a
            >
            - High-level API for specifying neural networks of both finite and
            <em>infinite</em> width.
            <img
              src="https://img.shields.io/github/stars/google/neural-tangents?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/huggingface/transformers"
              >HuggingFace</a
            >
            - Ecosystem of pretrained Transformers for a wide range of natural
            language tasks (Flax).
            <img
              src="https://img.shields.io/github/stars/huggingface/transformers?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/patrick-kidger/equinox">Equinox</a> -
            Callable PyTrees and filtered JIT/grad transformations =&gt; neural
            networks in JAX.
            <img
              src="https://img.shields.io/github/stars/patrick-kidger/equinox?style=social"
              align="center"
            />
          </li>
        </ul>
      </li>
      <li>
        <a href="https://github.com/pyro-ppl/numpyro">NumPyro</a> -
        Probabilistic programming based on the Pyro library.
        <img
          src="https://img.shields.io/github/stars/pyro-ppl/numpyro?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/deepmind/chex">Chex</a> - Utilities to write
        and test reliable JAX code.
        <img
          src="https://img.shields.io/github/stars/deepmind/chex?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/deepmind/optax">Optax</a> - Gradient
        processing and optimization library.
        <img
          src="https://img.shields.io/github/stars/deepmind/optax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/deepmind/rlax">RLax</a> - Library for
        implementing reinforcement learning agents.
        <img
          src="https://img.shields.io/github/stars/deepmind/rlax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/google/jax-md">JAX, M.D.</a> - Accelerated,
        differential molecular dynamics.
        <img
          src="https://img.shields.io/github/stars/google/jax-md?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/coax-dev/coax">Coax</a> - Turn RL papers
        into code, the easy way.
        <img
          src="https://img.shields.io/github/stars/coax-dev/coax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/SymJAX/SymJAX">SymJAX</a> - Symbolic
        CPU/GPU/TPU programming.
        <img
          src="https://img.shields.io/github/stars/SymJAX/SymJAX?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/rlouf/mcx">mcx</a> - Express &amp; compile
        probabilistic programs for performant inference.
        <img
          src="https://img.shields.io/github/stars/rlouf/mcx?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/deepmind/distrax">Distrax</a> -
        Reimplementation of TensorFlow Probability, containing probability
        distributions and bijectors.
        <img
          src="https://img.shields.io/github/stars/deepmind/distrax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/cvxgrp/cvxpylayers">cvxpylayers</a> -
        Construct differentiable convex optimization layers.
        <img
          src="https://img.shields.io/github/stars/cvxgrp/cvxpylayers?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/tensorly/tensorly">TensorLy</a> - Tensor
        learning made simple.
        <img
          src="https://img.shields.io/github/stars/tensorly/tensorly?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/netket/netket">NetKet</a> - Machine Learning
        toolbox for Quantum Physics.
        <img
          src="https://img.shields.io/github/stars/netket/netket?style=social"
          align="center"
        />
      </li>
    </ul>
    <p><a name="new-libraries" /></p>
    <h3 id="new-libraries">New Libraries</h3>
    <p>
      This section contains libraries that are well-made and useful, but have
      not necessarily been battle-tested by a large userbase yet.
    </p>
    <ul>
      <li>
        Neural Network Libraries
        <ul>
          <li>
            <a href="https://github.com/google/fedjax">FedJAX</a> - Federated
            learning in JAX, built on Optax and Haiku.
            <img
              src="https://img.shields.io/github/stars/google/fedjax?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/mfinzi/equivariant-MLP"
              >Equivariant MLP</a
            >
            - Construct equivariant neural network layers.
            <img
              src="https://img.shields.io/github/stars/mfinzi/equivariant-MLP?style=social"
              align="center"
            />
          </li>
          <li>
            <a href="https://github.com/n2cholas/jax-resnet/">jax-resnet</a> -
            Implementations and checkpoints for ResNet variants in Flax.
            <img
              src="https://img.shields.io/github/stars/n2cholas/jax-resnet?style=social"
              align="center"
            />
          </li>
        </ul>
      </li>
      <li>
        <a href="https://github.com/ElArkk/jax-unirep">jax-unirep</a> - Library
        implementing the
        <a href="https://www.nature.com/articles/s41592-019-0598-1"
          >UniRep model</a
        >
        for protein machine learning applications.
        <img
          src="https://img.shields.io/github/stars/ElArkk/jax-unirep?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/ChrisWaites/jax-flows">jax-flows</a> -
        Normalizing flows in JAX.
        <img
          src="https://img.shields.io/github/stars/ChrisWaites/jax-flows?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/ExpectationMax/sklearn-jax-kernels"
          >sklearn-jax-kernels</a
        >
        - <code>scikit-learn</code> kernel matrices using JAX.
        <img
          src="https://img.shields.io/github/stars/ExpectationMax/sklearn-jax-kernels?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo"
          >jax-cosmo</a
        >
        - Differentiable cosmology library.
        <img
          src="https://img.shields.io/github/stars/DifferentiableUniverseInitiative/jax_cosmo?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/NeilGirdhar/efax">efax</a> - Exponential
        Families in JAX.
        <img
          src="https://img.shields.io/github/stars/NeilGirdhar/efax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/PhilipVinc/mpi4jax">mpi4jax</a> - Combine
        MPI operations with your Jax code on CPUs and GPUs.
        <img
          src="https://img.shields.io/github/stars/PhilipVinc/mpi4jax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/4rtemi5/imax">imax</a> - Image augmentations
        and transformations.
        <img
          src="https://img.shields.io/github/stars/4rtemi5/imax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/rolandgvc/flaxvision">FlaxVision</a> - Flax
        version of TorchVision.
        <img
          src="https://img.shields.io/github/stars/rolandgvc/flaxvision?style=social"
          align="center"
        />
      </li>
      <li>
        <a
          href="https://github.com/tensorflow/probability/tree/master/spinoffs/oryx"
          >Oryx</a
        >
        - Probabilistic programming language based on program transformations.
      </li>
      <li>
        <a href="https://github.com/google-research/ott"
          >Optimal Transport Tools</a
        >
        - Toolbox that bundles utilities to solve optimal transport problems.
      </li>
      <li>
        <a href="https://github.com/romanodev/deltapv">delta PV</a> - A
        photovoltaic simulator with automatic differentation.
        <img
          src="https://img.shields.io/github/stars/romanodev/deltapv?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/brentyi/jaxlie">jaxlie</a> - Lie theory
        library for rigid body transformations and optimization.
        <img
          src="https://img.shields.io/github/stars/brentyi/jaxlie?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/google/brax">BRAX</a> - Differentiable
        physics engine to simulate environments along with learning algorithms
        to train agents for these environments.
        <img
          src="https://img.shields.io/github/stars/google/brax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/matthias-wright/flaxmodels">flaxmodels</a> -
        Pretrained models for Jax/Flax.
        <img
          src="https://img.shields.io/github/stars/matthias-wright/flaxmodels?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/carnotresearch/cr-sparse">CR.Sparse</a> -
        XLA accelerated algorithms for sparse representations and compressive
        sensing.
        <img
          src="https://img.shields.io/github/stars/carnotresearch/cr-sparse?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/HajimeKawahara/exojax">exojax</a> -
        Automatic differentiable spectrum modeling of exoplanets/brown dwarfs
        compatible to JAX.
        <img
          src="https://img.shields.io/github/stars/HajimeKawahara/exojax?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/google/jaxopt">JAXopt</a> - Hardware
        accelerated (GPU/TPU), batchable and differentiable optimizers in JAX.
        <img
          src="https://img.shields.io/github/stars/google/jaxopt?style=social"
          align="center"
        />
      </li>
      <li>
        <a href="https://github.com/deepmind/dm_pix">PIX</a> - PIX is an image
        processing library in JAX, for JAX.
        <img
          src="https://img.shields.io/github/stars/deepmind/dm_pix?style=social"
          align="center"
        />
      </li>
    </ul>
    <p><a name="models-and-projects" /></p>
    <h2 id="models-and-projects">Models and Projects</h2>
    <h3 id="jax">JAX</h3>
    <ul>
      <li>
        <a href="https://github.com/tancik/fourier-feature-networks"
          >Fourier Feature Networks</a
        >
        - Official implementation of
        <a href="https://people.eecs.berkeley.edu/~bmild/fourfeat"
          ><em
            >Fourier Features Let Networks Learn High Frequency Functions in Low
            Dimensional Domains</em
          ></a
        >.
      </li>
      <li>
        <a href="https://github.com/AaltoML/kalman-jax">kalman-jax</a> -
        Approximate inference for Markov (i.e., temporal) Gaussian processes
        using iterated Kalman filtering and smoothing.
      </li>
      <li>
        <a href="https://github.com/thomaspinder/GPJax">GPJax</a> - Gaussian
        processes in JAX.
      </li>
      <li>
        <a href="https://github.com/Joshuaalbert/jaxns">jaxns</a> - Nested
        sampling in JAX.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/amortized_bo"
          >Amortized Bayesian Optimization</a
        >
        - Code related to
        <a href="http://www.auai.org/uai2020/proceedings/329_main_paper.pdf"
          ><em>Amortized Bayesian Optimization over Discrete Spaces</em></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/aqt"
          >Accurate Quantized Training</a
        >
        - Tools and libraries for running and analyzing neural network
        quantization experiments in JAX and Flax.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/bnn_hmc"
          >BNN-HMC</a
        >
        - Implementation for the paper
        <a href="https://arxiv.org/abs/2104.14421"
          ><em>What Are Bayesian Neural Network Posteriors Really Like?</em></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/jax_dft"
          >JAX-DFT</a
        >
        - One-dimensional density functional theory (DFT) in JAX, with
        implementation of
        <a
          href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.036401"
          ><em
            >Kohn-Sham equations as regularizer: building prior knowledge into
            machine-learned physics</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/robust_loss_jax"
          >Robust Loss</a
        >
        - Reference code for the paper
        <a href="https://arxiv.org/abs/1701.03077"
          ><em>A General and Adaptive Robust Loss Function</em></a
        >.
      </li>
    </ul>
    <h3 id="flax">Flax</h3>
    <ul>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/performer/fast_attention/jax"
          >Performer</a
        >
        - Flax implementation of the Performer (linear transformer via FAVOR+)
        architecture.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/jaxnerf"
          >JaxNeRF</a
        >
        - Implementation of
        <a href="http://www.matthewtancik.com/nerf"
          ><em
            >NeRF: Representing Scenes as Neural Radiance Fields for View
            Synthesis</em
          ></a
        >
        with multi-device GPU/TPU support.
      </li>
      <li>
        <a href="https://github.com/google-research/big_transfer"
          >Big Transfer (BiT)</a
        >
        - Implementation of
        <a href="https://arxiv.org/abs/1912.11370"
          ><em
            >Big Transfer (BiT): General Visual Representation Learning</em
          ></a
        >.
      </li>
      <li>
        <a href="https://github.com/ikostrikov/jax-rl">JAX RL</a> -
        Implementations of reinforcement learning algorithms.
      </li>
      <li>
        <a href="https://github.com/SauravMaheshkar/gMLP">gMLP</a> -
        Implementation of
        <a href="https://arxiv.org/abs/2105.08050"
          ><em>Pay Attention to MLPs</em></a
        >.
      </li>
      <li>
        <a href="https://github.com/SauravMaheshkar/MLP-Mixer">MLP Mixer</a> -
        Minimal implementation of
        <a href="https://arxiv.org/abs/2105.01601"
          ><em>MLP-Mixer: An all-MLP Architecture for Vision</em></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/scalable_shampoo"
          >Distributed Shampoo</a
        >
        - Implementation of
        <a href="https://arxiv.org/abs/2002.09018"
          ><em>Second Order Optimization Made Practical</em></a
        >.
      </li>
      <li>
        <a href="https://github.com/google-research/nested-transformer">NesT</a>
        - Official implementation of
        <a href="https://arxiv.org/abs/2105.12723"
          ><em>Aggregating Nested Transformers</em></a
        >.
      </li>
      <li>
        <a href="https://github.com/google-research/xmcgan_image_generation"
          >XMC-GAN</a
        >
        - Official implementation of
        <a href="https://arxiv.org/abs/2101.04702"
          ><em
            >Cross-Modal Contrastive Learning for Text-to-Image Generation</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/f_net"
          >FNet</a
        >
        - Official implementation of
        <a href="https://arxiv.org/abs/2105.03824"
          ><em>FNet: Mixing Tokens with Fourier Transforms</em></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/gfsa"
          >GFSA</a
        >
        - Official implementation of
        <a href="https://arxiv.org/abs/2007.04929"
          ><em
            >Learning Graph Structure With A Finite-State Automaton Layer</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/ipagnn"
          >IPA-GNN</a
        >
        - Official implementation of
        <a href="https://arxiv.org/abs/2010.12621"
          ><em
            >Learning to Execute Programs with Instruction Pointer Attention
            Graph Neural Networks</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/flax_models"
          >Flax Models</a
        >
        - Collection of models and methods implemented in Flax.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/protein_lm"
          >Protein LM</a
        >
        - Implements BERT and autoregressive models for proteins, as described
        in
        <a href="https://www.biorxiv.org/content/10.1101/622803v1.full"
          ><em
            >Biological Structure and Function Emerge from Scaling Unsupervised
            Learning to 250 Million Protein Sequences</em
          ></a
        >
        and
        <a href="https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2"
          ><em>ProGen: Language Modeling for Protein Generation</em></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/ptopk_patch_selection"
          >Slot Attention</a
        >
        - Reference implementation for
        <a href="https://arxiv.org/abs/2104.03059"
          ><em>Differentiable Patch Selection for Image Recognition</em></a
        >.
      </li>
      <li>
        <a href="https://github.com/google-research/vision_transformer"
          >Vision Transformer</a
        >
        - Official implementation of
        <a href="https://arxiv.org/abs/2010.11929"
          ><em
            >An Image is Worth 16x16 Words: Transformers for Image Recognition
            at Scale</em
          ></a
        >.
      </li>
      <li>
        <a href="https://github.com/matthias-wright/jax-fid">FID computation</a>
        - Port of
        <a href="https://github.com/mseitzer/pytorch-fid"
          >mseitzer/pytorch-fid</a
        >
        to Flax.
      </li>
    </ul>
    <h3 id="haiku">Haiku</h3>
    <ul>
      <li>
        <a href="https://github.com/deepmind/alphafold">AlphaFold</a> -
        Implementation of the inference pipeline of AlphaFold v2.0, presented in
        <a href="https://www.nature.com/articles/s41586-021-03819-2"
          ><em
            >Highly accurate protein structure prediction with AlphaFold</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness"
          >Adversarial Robustness</a
        >
        - Reference code for
        <a href="https://arxiv.org/abs/2010.03593"
          ><em
            >Uncovering the Limits of Adversarial Training against Norm-Bounded
            Adversarial Examples</em
          ></a
        >
        and
        <a href="https://arxiv.org/abs/2103.01946"
          ><em
            >Fixing Data Augmentation to Improve Adversarial Robustness</em
          ></a
        >.
      </li>
      <li>
        <a href="https://github.com/deepmind/deepmind-research/tree/master/byol"
          >Bootstrap Your Own Latent</a
        >
        - Implementation for the paper
        <a href="https://arxiv.org/abs/2006.07733"
          ><em
            >Bootstrap your own latent: A new approach to self-supervised
            Learning</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/deepmind/deepmind-research/tree/master/gated_linear_networks"
          >Gated Linear Networks</a
        >
        - GLNs are a family of backpropagation-free neural networks.
      </li>
      <li>
        <a
          href="https://github.com/deepmind/deepmind-research/tree/master/glassy_dynamics"
          >Glassy Dynamics</a
        >
        - Open source implementation of the paper
        <a href="https://www.nature.com/articles/s41567-020-0842-8"
          ><em
            >Unveiling the predictive power of static structure in glassy
            systems</em
          ></a
        >.
      </li>
      <li>
        <a href="https://github.com/deepmind/deepmind-research/tree/master/mmv"
          >MMV</a
        >
        - Code for the models in
        <a href="https://arxiv.org/abs/2006.16228"
          ><em>Self-Supervised MultiModal Versatile Networks</em></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/deepmind/deepmind-research/tree/master/nfnets"
          >Normalizer-Free Networks</a
        >
        - Official Haiku implementation of
        <a href="https://arxiv.org/abs/2102.06171"><em>NFNets</em></a
        >.
      </li>
      <li>
        <a href="https://github.com/Information-Fusion-Lab-Umass/NuX">NuX</a> -
        Normalizing flows with JAX.
      </li>
      <li>
        <a
          href="https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc"
          >OGB-LSC</a
        >
        - This repository contains DeepMind’s entry to the
        <a href="https://ogb.stanford.edu/kddcup2021/pcqm4m/">PCQM4M-LSC</a>
        (quantum chemistry) and
        <a href="https://ogb.stanford.edu/kddcup2021/mag240m/">MAG240M-LSC</a>
        (academic graph) tracks of the
        <a href="https://ogb.stanford.edu/kddcup2021/"
          >OGB Large-Scale Challenge</a
        >
        (OGB-LSC).
      </li>
      <li>
        <a
          href="https://github.com/google-research/google-research/tree/master/persistent_es"
          >Persistent Evolution Strategies</a
        >
        - Code used for the paper
        <a href="http://proceedings.mlr.press/v139/vicol21a.html"
          ><em
            >Unbiased Gradient Estimation in Unrolled Computation Graphs with
            Persistent Evolution Strategies</em
          ></a
        >.
      </li>
      <li>
        <a
          href="https://github.com/deepmind/deepmind-research/tree/master/wikigraphs"
          >WikiGraphs</a
        >
        - Baseline code to reproduce results in
        <a href="https://aclanthology.org/2021.textgraphs-1.7"
          ><em
            >WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Datase</em
          ></a
        >.
      </li>
    </ul>
    <h3 id="trax">Trax</h3>
    <ul>
      <li>
        <a
          href="https://github.com/google/trax/tree/master/trax/models/reformer"
          >Reformer</a
        >
        - Implementation of the Reformer (efficient transformer) architecture.
      </li>
    </ul>
    <p><a name="videos" /></p>
    <h2 id="videos">Videos</h2>
    <ul>
      <li>
        <a href="https://www.youtube.com/watch?v=iDxJxIyzSiM"
          >NeurIPS 2020: JAX Ecosystem Meetup</a
        >
        - JAX, its use at DeepMind, and discussion between engineers,
        scientists, and JAX core team.
      </li>
      <li>
        <a href="https://youtu.be/0mVmRHMaOJ4">Introduction to JAX</a> - Simple
        neural network from scratch in JAX.
      </li>
      <li>
        <a href="https://youtu.be/z-WSrQDXkuM"
          >JAX: Accelerated Machine Learning Research | SciPy 2020 |
          VanderPlas</a
        >
        - JAX’s core design, how it’s powering new research, and how you can
        start using it.
      </li>
      <li>
        <a href="https://youtu.be/CecuWGpoztw"
          >Bayesian Programming with JAX + NumPyro — Andy Kitchen</a
        >
        - Introduction to Bayesian modelling using NumPyro.
      </li>
      <li>
        <a
          href="https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python"
          >JAX: Accelerated machine-learning research via composable function
          transformations in Python | NeurIPS 2019 | Skye Wanderman-Milne</a
        >
        - JAX intro presentation in
        <a href="https://program-transformations.github.io"
          ><em>Program Transformations for Machine Learning</em></a
        >
        workshop.
      </li>
      <li>
        <a
          href="https://drive.google.com/file/d/1jKxefZT1xJDUxMman6qrQVed7vWI0MIn/edit"
          >JAX on Cloud TPUs | NeurIPS 2020 | Skye Wanderman-Milne and James
          Bradbury</a
        >
        - Presentation of TPU host access with demo.
      </li>
      <li>
        <a
          href="https://slideslive.com/38935810/deep-implicit-layers-neural-odes-equilibrium-models-and-beyond"
          >Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and
          Beyond | NeurIPS 2020</a
        >
        - Tutorial created by Zico Kolter, David Duvenaud, and Matt Johnson with
        Colab notebooks avaliable in
        <a href="http://implicit-layers-tutorial.org"
          ><em>Deep Implicit Layers</em></a
        >.
      </li>
      <li>
        <a href="http://matpalm.com/blog/ymxb_pod_slice/"
          >Solving y=mx+b with Jax on a TPU Pod slice - Mat Kelcey</a
        >
        - A four part YouTube tutorial series with Colab notebooks that starts
        with Jax fundamentals and moves up to training with a data parallel
        approach on a v3-32 TPU Pod slice.
      </li>
      <li>
        <a
          href="https://github.com/huggingface/transformers/blob/9160d81c98854df44b1d543ce5d65a6aa28444a2/examples/research_projects/jax-projects/README.md#talks"
          >JAX, Flax &amp; Transformers 🤗</a
        >
        - 3 days of talks around JAX / Flax, Transformers, large-scale language
        modeling and other great topics.
      </li>
    </ul>
    <p><a name="papers" /></p>
    <h2 id="papers">Papers</h2>
    <p>
      This section contains papers focused on JAX (e.g. JAX-based library
      whitepapers, research on JAX, etc). Papers implemented in JAX are listed
      in the <a href="#projects">Models/Projects</a> section.
    </p>
    <!--lint ignore awesome-list-item-->
    <ul>
      <li>
        <a href="https://mlsys.org/Conferences/doc/2018/146.pdf"
          ><strong
            >Compiling machine learning programs via high-level tracing</strong
          >. Roy Frostig, Matthew James Johnson, Chris Leary.
          <em>MLSys 2018</em>.</a
        >
        - White paper describing an early version of JAX, detailing how
        computation is traced and compiled.
      </li>
      <li>
        <a href="https://arxiv.org/abs/1912.04232"
          ><strong>JAX, M.D.: A Framework for Differentiable Physics</strong>.
          Samuel S. Schoenholz, Ekin D. Cubuk. <em>NeurIPS 2020</em>.</a
        >
        - Introduces JAX, M.D., a differentiable physics library which includes
        simulation environments, interaction potentials, neural networks, and
        more.
      </li>
      <li>
        <a href="https://arxiv.org/abs/2010.09063"
          ><strong
            >Enabling Fast Differentially Private SGD via Just-in-Time
            Compilation and Vectorization</strong
          >. Pranav Subramani, Nicholas Vadivelu, Gautam Kamath.
          <em>arXiv 2020</em>.</a
        >
        - Uses JAX’s JIT and VMAP to achieve faster differentially private than
        existing libraries.
        <!--lint enable awesome-list-item-->
      </li>
    </ul>
    <p><a name="tutorials-and-blog-posts" /></p>
    <h2 id="tutorials-and-blog-posts">Tutorials and Blog Posts</h2>
    <ul>
      <li>
        <a
          href="https://deepmind.com/blog/article/using-jax-to-accelerate-our-research"
          >Using JAX to accelerate our research by David Budden and Matteo
          Hessel</a
        >
        - Describes the state of JAX and the JAX ecosystem at DeepMind.
      </li>
      <li>
        <a href="https://roberttlange.github.io/posts/2020/03/blog-post-10/"
          >Getting started with JAX (MLPs, CNNs &amp; RNNs) by Robert Lange</a
        >
        - Neural network building blocks from scratch with the basic JAX
        operators.
      </li>
      <li>
        <a
          href="https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen"
          >Tutorial: image classification with JAX and Flax Linen by 8bitmp3</a
        >
        - Learn how to create a simple convolutional network with the Linen API
        by Flax and train it to recognize handwritten digits.
      </li>
      <li>
        <a href="https://medium.com/swlh/plugging-into-jax-16c120ec3302"
          >Plugging Into JAX by Nick Doiron</a
        >
        - Compares Flax, Haiku, and Objax on the Kaggle flower classification
        challenge.
      </li>
      <li>
        <a href="https://blog.evjang.com/2019/02/maml-jax.html"
          >Meta-Learning in 50 Lines of JAX by Eric Jang</a
        >
        - Introduction to both JAX and Meta-Learning.
      </li>
      <li>
        <a href="https://blog.evjang.com/2019/07/nf-jax.html"
          >Normalizing Flows in 100 Lines of JAX by Eric Jang</a
        >
        - Concise implementation of
        <a href="https://arxiv.org/abs/1605.08803">RealNVP</a>.
      </li>
      <li>
        <a href="https://blog.evjang.com/2019/11/jaxpt.html"
          >Differentiable Path Tracing on the GPU/TPU by Eric Jang</a
        >
        - Tutorial on implementing path tracing.
      </li>
      <li>
        <a href="http://matpalm.com/blog/ensemble_nets"
          >Ensemble networks by Mat Kelcey</a
        >
        - Ensemble nets are a method of representing an ensemble of models as
        one single logical model.
      </li>
      <li>
        <a href="http://matpalm.com/blog/ood_using_focal_loss"
          >Out of distribution (OOD) detection by Mat Kelcey</a
        >
        - Implements different methods for OOD detection.
      </li>
      <li>
        <a href="https://www.radx.in/jax.html"
          >Understanding Autodiff with JAX by Srihari Radhakrishna</a
        >
        - Understand how autodiff works using JAX.
      </li>
      <li>
        <a href="https://sjmielke.com/jax-purify.htm"
          >From PyTorch to JAX: towards neural net frameworks that purify
          stateful code by Sabrina J. Mielke</a
        >
        - Showcases how to go from a PyTorch-like style of coding to a more
        Functional-style of coding.
      </li>
      <li>
        <a href="https://github.com/dfm/extending-jax"
          >Extending JAX with custom C++ and CUDA code by Dan Foreman-Mackey</a
        >
        - Tutorial demonstrating the infrastructure required to provide custom
        ops in JAX.
      </li>
      <li>
        <a href="https://roberttlange.github.io/posts/2021/02/cma-es-jax/"
          >Evolving Neural Networks in JAX by Robert Tjarko Lange</a
        >
        - Explores how JAX can power the next generation of scalable
        neuroevolution algorithms.
      </li>
      <li>
        <a
          href="http://lukemetz.com/exploring-hyperparameter-meta-loss-landscapes-with-jax/"
          >Exploring hyperparameter meta-loss landscapes with JAX by Luke
          Metz</a
        >
        - Demonstrates how to use JAX to perform inner-loss optimization with
        SGD and Momentum, outer-loss optimization with gradients, and outer-loss
        optimization using evolutionary strategies.
      </li>
      <li>
        <a href="https://martiningram.github.io/deterministic-advi/"
          >Deterministic ADVI in JAX by Martin Ingram</a
        >
        - Walk through of implementing automatic differentiation variational
        inference (ADVI) easily and cleanly with JAX.
      </li>
      <li>
        <a href="http://matpalm.com/blog/evolved_channel_selection/"
          >Evolved channel selection by Mat Kelcey</a
        >
        - Trains a classification model robust to different combinations of
        input channels at different resolutions, then uses a genetic algorithm
        to decide the best combination for a particular loss.
      </li>
      <li>
        <a
          href="https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/jax_intro.ipynb"
          >Introduction to JAX by Kevin Murphy</a
        >
        - Colab that introduces various aspects of the language and applies them
        to simple ML problems.
      </li>
      <li>
        <a href="https://www.jeremiecoullon.com/2020/11/10/mcmcjax3ways/"
          >Writing an MCMC sampler in JAX by Jeremie Coullon</a
        >
        - Tutorial on the different ways to write an MCMC sampler in JAX along
        with speed benchmarks.
      </li>
      <li>
        <a href="https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/"
          >How to add a progress bar to JAX scans and loops by Jeremie
          Coullon</a
        >
        - Tutorial on how to add a progress bar to compiled loops in JAX using
        the <code>host_callback</code> module.
      </li>
    </ul>
    <p><a name="community" /></p>
    <h2 id="community">Community</h2>
    <ul>
      <li>
        <a href="https://github.com/google/jax/discussions"
          >JAX GitHub Discussions</a
        >
      </li>
      <li><a href="https://www.reddit.com/r/JAX/">Reddit</a></li>
    </ul>
    <h2 id="contributing">Contributing</h2>
    <p>
      Contributions welcome! Read the
      <a href="contributing.md">contribution guidelines</a> first.
    </p>
  </body>
</html>
