<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
</head>
<body>
<h1 id="machine-learning">Machine Learning</h1>
<h2 id="external-papers">External Papers</h2>
<ul>
<li><p><a href="https://www.researchgate.net/publication/29467751_Top_10_algorithms_in_data_mining">Top 10 algorithms in data mining</a></p>
<p>While it is difficult to identify the top 10, this paper contains 10 very important data mining/machine learning algorithms</p></li>
<li><p><a href="http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Know about Machine Learning</a></p>
Just like the title says, it contains many useful tips and gotchas for machine learning</li>
<li><p><a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">Random Forests</a></p>
The initial paper on random forests</li>
<li><p><a href="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;context=cis_papers">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</a></p>
The paper introducing conditional random fields as a framework for building probabilistic models.</li>
<li><p><a href="http://rd.springer.com/content/pdf/10.1007%2FBF00994018.pdf">Support-Vector Networks</a></p>
<p>The initial paper on support-vector networks for classification.</p></li>
<li><p><a href="https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf">The Fast Johnson-Lindenstrauss Transforms</a></p>
<p>The Johnson-Lindenstrauss transform (JLT) prescribes that there exists a matrix of size <code>k x d</code>, where <code>k = O(1/eps^2 log d)</code> such that with high probability, a matrix A drawn from this distribution preserves pairwise distances up to epsilon (e.g. <code>(1-eps) * ||x-y|| &lt; ||Ax - Ay|| &lt; (1+eps) ||x-y||</code>). This paper was the first paper to show that you can actually compute the JLT in less that <code>O(kd)</code> operations (e.g. you don’t need to do the full matrix multiplication). They used their faster algorithm to construct one of the fastest known approximate nearest neighbor algorithms.</p>
<p><em>Ailon, Nir, and Bernard Chazelle. “The fast Johnson-Lindenstrauss transform and approximate nearest neighbors.” SIAM Journal on Computing 39.1 (2009): 302-322. Available: https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf</em></p></li>
<li><p><a href="http://www.berkkapicioglu.com/wp-content/uploads/2013/11/thesis_final.pdf">Applications of Machine Learning to Location Data</a></p>
<p>Using machine learning to design and analyze novel algorithms that leverage location data.</p></li>
<li><p><a href="http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf">“Why Should I Trust You?” Explaining the Predictions of Any Classifier</a></p>
<p>This paper introduces an explanation technique for any classifier in a interpretable manner.</p></li>
<li><p><a href="http://dreammachin.es/p1-wallace.pdf">Multiple Narrative Disentanglement: Unraveling <em>Inﬁnite Jest</em></a></p>
<p>Uses an unsupervised approach to natural language processing that classifies narrators in David Foster Wallace’s 1,000-page novel.</p></li>
<li><p><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>This paper introduces AlexNet, a neural network architecture which dramatically improved over the state-of-the-art in image classification algorithms and is widely regarded as a breakthrough moment for deep learning.</p></li>
<li><p><a href="https://arxiv.org/pdf/1901.04592.pdf">Interpretable machine learning: definitions, methods, and applications</a></p>
<p>This paper introduces the foundations of the rapidly emerging field of interpretable machine learning.</p></li>
<li><p><a href="https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a></p>
<p>This seminal paper introduces a method to distill information from an ensemble of neural networks into a single model.</p></li>
<li><p><a href="https://reader.elsevier.com/reader/sd/pii/0024379594000395?token=EB0AA78D59A9648480596F018EFB72E0A02FD5FA70326B24B9D501E1A6869FE72CC4D97FA9ACC8BAB56060D6C908EC83">Truncation of Wavelet Matrices: Edge Effects and the Reduction of Topological Control</a> by Freedman</p>
<p>In this paper by Michael Hartley Freedman, he applies Robion Kirby “torus trick”, via wavelets, to the problem of compression.</p></li>
</ul>
<h2 id="hosted-papers">Hosted Papers</h2>
<ul>
<li><p>:scroll: <strong><a href="dimensionality_reduction/a-sparse-johnson-lindenstrauss-transform.pdf">A Sparse Johnson-Lindenstrauss Transform</a></strong></p>
<p>The JLT is still computationally expensive for a lot of applications and one goal would be to minimize the overall operations needed to do the aforementioned matrix multiplication. This paper showed that a goal of a <code>O(k log d)</code> algorithm (e.g. <code>(log(d))^2)</code> may be attainable by showing that very sparse, structured random matrices could provide the <em>JL</em> guarantee on pairwise distances.</p>
<p><em>Dasgupta, Anirban, Ravi Kumar, and Tamás Sarlós. “A sparse johnson: Lindenstrauss transform.” Proceedings of the forty-second ACM symposium on Theory of computing. ACM, 2010. Available: <a href="http://arxiv.org/abs/1004.4240">arXiv/cs/1004:4240</a></em></p></li>
<li><p>:scroll: <strong><a href="dimensionality_reduction/toward-a-unified-theory-of-sparse-dimensionality-reduction-in-euclidean-space.pdf">Towards a unified theory of sparse dimensionality reduction in Euclidean space</a></strong></p>
<p>This paper attempts to layout the generic mathematical framework (in terms of convex analysis and functional analysis) for sparse dimensionality reduction. The first author is a Fields Medalist who is interested in taking techniques for Banach Spaces and applying them to this problem. This paper is a very technical paper that attempts to answer the question, “when does a sparse embedding exist deterministically?” (e.g. doesn’t require drawing random matrices).</p>
<p><em>Bourgain, Jean, and Jelani Nelson. “Toward a unified theory of sparse dimensionality reduction in euclidean space.” arXiv preprint arXiv:1311.2542; Accepted in an AMS Journal but unpublished at the moment (2013). Available: http://arxiv.org/abs/1311.2542</em></p></li>
<li><p>:scroll: <strong><a href="Understanding-Deep-Convolutional-Networks.pdf">Understanding Deep Convolutional Networks</a></strong> by Mallat</p>
<p>Stéphane Mallat proposes a model by which renormalisation can identify self-similar structures in deep networks. <a href="https://www.youtube.com/watch?v=_qjPFF5Gv1I">This video of Curt MacMullen discussing renormalization</a> can help with more context.</p></li>
<li><p>:scroll: <strong><a href="General-self-similarity--an-overview.pdf">General self-similarity: an overview</a></strong> by Leinster</p>
<p>Dr. Leinster’s paper provides a concise, straightforward, picture of self-similarity, and its role in renormalization.</p></li>
</ul>
</body>
</html>
